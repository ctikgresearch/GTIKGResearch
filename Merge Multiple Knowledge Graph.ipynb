{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "file_path = 'triples.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "df.drop_duplicates(subset='single_article', keep='last', inplace=True)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triples(input_longmemstr):\n",
    "    final_triples = []\n",
    "    import re\n",
    "    def extract_content(text):\n",
    "        text = text.replace(\"'\", \"\")\n",
    "        text = text.replace('\"', \"\")\n",
    "        text = text.replace('\\n', \"\")\n",
    "        text = text.replace(':  ', \"\")\n",
    "        text = text.replace(': ', \"\")\n",
    "        text = text.replace(' :', \"\")\n",
    "        text = re.sub(r\"(SUBJECT:|Subject:|OBJECT:|RELATION:)\", \"\", text)\n",
    "        pattern = r\"\\[(.*?,.*?,.*?)\\]\"\n",
    "        matches = re.findall(pattern, text)\n",
    "        return matches\n",
    "\n",
    "    input_longmem_listsrt = extract_content(input_longmemstr)\n",
    "    import ast\n",
    "\n",
    "    for i in input_longmem_listsrt:\n",
    "        try:\n",
    "            i = i.replace(',', '\",\"')\n",
    "            i = '[\"' + (i) + '\"]'\n",
    "            result = ast.literal_eval(i)\n",
    "            final_triples.append(result)\n",
    "        except:\n",
    "            print('error')\n",
    "            continue\n",
    "\n",
    "    for tri in final_triples:\n",
    "        for i in range(len(tri)):\n",
    "            tri[i] = tri[i].strip()\n",
    "\n",
    "    return final_triples\n",
    "\n",
    "def build_graph_allsentences(df):\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(1, len(df)):\n",
    "        fulltext=df.loc[i,'single_article']\n",
    "        longmemstr=df.loc[i,'longmem']\n",
    "        triples = get_triples(longmemstr)\n",
    "        for triple in triples:\n",
    "            source = triple[0]\n",
    "            target = triple[2]\n",
    "            G.add_edge(source, target, articletext=fulltext, rel=triple[1])\n",
    "    return G\n",
    "\n",
    "def build_graph_alltriples(dfin):\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(0, len(dfin)):\n",
    "        fulltext = dfin.loc[i,'fulltext']\n",
    "        subw = dfin.loc[i,'sub']\n",
    "        relw = dfin.loc[i,'rel']\n",
    "        objw = dfin.loc[i,'obj']\n",
    "        subtype = dfin.loc[i,'subtype']\n",
    "        tactic = dfin.loc[i,'tactic']\n",
    "        objtype = dfin.loc[i,'objtype']\n",
    "        fulltext = dfin.loc[i,'fulltext']\n",
    "        be = dfin.loc[i,'be']\n",
    "        G.add_node(subw, entity=subtype)\n",
    "        G.add_node(objw, entity=objtype)\n",
    "        G.add_edge(subw, objw, article_id=fulltext, rel=relw, tactic=tactic,behave_conf=be)\n",
    "    return G\n",
    "\n",
    "from simpletransformers.language_representation import RepresentationModel\n",
    "from simpletransformers.config.model_args import ModelArgs\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_most_similar(given_string, string_list):\n",
    "    model_args = ModelArgs(max_seq_length=156)\n",
    "\n",
    "    model = RepresentationModel(\n",
    "        \"roberta\",\n",
    "        \"./tactic model\",\n",
    "        args=model_args,\n",
    "    )\n",
    "\n",
    "    given_embedding = model.encode_sentences([given_string], combine_strategy=\"mean\")\n",
    "\n",
    "    similarities = []\n",
    "    for s in string_list:\n",
    "        s_embedding = model.encode_sentences([s], combine_strategy=\"mean\")\n",
    "        cosine_sim = cosine_similarity(given_embedding, s_embedding)\n",
    "        similarities.append((s, cosine_sim[0][0]))\n",
    "\n",
    "    top_5 = sorted(similarities, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    return [s[0] for s in top_5]\n",
    "\n",
    "def ask(prompt, token, temp, model=\"TheBloke/Yi-34B-Chat-AWQ\",streamprint=True):\n",
    "    import os\n",
    "    from openai import OpenAI\n",
    "    if model == \"gpt4\":\n",
    "        os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        api_base = 'https://api.openai.com/v1'\n",
    "        setmodel = 'gpt-4-0125-preview'\n",
    "    else:\n",
    "        setmodel=model\n",
    "        api_key = \"EMPTY\"\n",
    "        api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "    print(\"Model:\", setmodel)\n",
    "    stream = client.chat.completions.create(\n",
    "        model=setmodel,\n",
    "        messages=prompt,\n",
    "        stream=True,\n",
    "        max_tokens=token,\n",
    "        temperature=temp,\n",
    "        extra_body={\n",
    "        \"stop_token_ids\": [7]\n",
    "        }\n",
    "    )\n",
    "    final_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            if streamprint:\n",
    "                print(chunk.choices[0].delta.content, end=\"\")\n",
    "            final_response += chunk.choices[0].delta.content  \n",
    "    return final_response                \n",
    "        \n",
    "def generate_prompt(longmem,shortmem,sentence):\n",
    "    promptmessage = [\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \n",
    "    '''You are a triples integration assistant. Triple is a basic data structure, which describes concepts and their relationships. A triple in long-term and short-term memory MUST has THREE elements: [Subject, Relation, Object]. You are now reading a whole article and extract all triples from it. But you can only see part of the article at a time, and the entity names in the article are not always consistent, for example, \"the Formbook\" and \"Formbook sample\" refer to the same entity. In order to make the triples consistent, you need to follow the rules to modify the triples in short-term memory and then write down new short-term memory. \n",
    "    You have a long-term memory that already contains some triples which are consistent with each other.\n",
    "    -The start of the long-term memory area-\n",
    "    #Triples will be added here\n",
    "    -The end of the short-term memory area-\n",
    "    Second, you now see a short term memory area. It contains only one triple extracted from the article. You should modify this triple in short-term memory to make them consistent with triples in long-term memory. \n",
    "    -The start of the short-term memory area-\n",
    "    #Triple will be added here\n",
    "    -The end of the short-term memory area-\n",
    "    Third, now review your long-term memory and short-term memory. Modify the short-term memory into a new short-term memory. You should follow following rules to modify triple in short-term memory to make them consistent with triples in long-term memory. You should write down how you use the rule to modify the triple in short-term memory. \n",
    "    Rule 1. You are only allowed to do small modifications to the triples in short-term memory, including deleting, adding, or modifying some words. You are not allowed to change the meaning of the triples. When you write down the new short-term memory, you should think wheather the new short-term memory explains the same meaning as the old short-term memory.\n",
    "    \n",
    "    Rule 2. You notice that in these triples, some triples have subjects and objects that contain partially identical terms and refer to the same specific nouns, but these specific nouns have prefixes/suffixes/modifiers that make them not identical. You should delete the prefixes/suffixes/modifiers and unify them into the same specific nouns.\n",
    "    \n",
    "    Before rule: [the Formbook, is designed to run as, a deleter] [Formbook sample, is designed to run as, one-time encryptor]\n",
    "\n",
    "    After rule: [Formbook, is designed to run as, a deleter] [Formbook, is designed to run as, one-time encryptor]\n",
    "\n",
    "    Explanation: The words \"the Formbook\" and \"Formbook sample\" refer to the same entity, so they are unified to use the exact same subject \"Formbook\" for consistency.\n",
    "    \n",
    "    Rule 3. Be especially careful that when you meet specific names of malware,CVE, Trojans, hacker organizations, etc., always use their specific names and remove the prefixes/suffixes/modifiers.\n",
    "    \n",
    "    Before rule: [Malware Formbook, is, malware] \n",
    "    \n",
    "    After rule: [Formbook, is, malware]\n",
    "    \n",
    "    Explanation: The word \"Formbook\" is a specific name of malware, so it should be used as the subject of the triple and the prefix \"Malware\" should be removed.\n",
    "    \n",
    "    Rule 4. Don't add any example word like 'Formbook','XLoader','Leafminer', 'FinSpy', 'Kismet' in your new short-term memory area, they are just example words not the real triples in the long term memory area or short term memory area.\n",
    "    \n",
    "    Rule 5. new short-term memory area must be started with \\'-The start of new short-term memory area-\\' and ended with \\'-The end of new short-term memory area-\\'. A triple in new short-term memory MUST has THREE elements: [Subject, Relation, Object]. \n",
    "\n",
    "    Rule 6. The new short-term memory area must only has one triple in it.\n",
    "    '''\n",
    "    },\n",
    "    {\"role\": \"assistant\",'content': 'Yes, I understand and totally follow the rules.'},\n",
    "    {\"role\": \"user\",'content': '''\n",
    "        -The start of the long-term memory area-  \n",
    "        [SUBJECT:AgumonHacker, RELATION:disrupts, OBJECT:network operations], [SUBJECT:HackGroup Gabumon, RELATION:targets, OBJECT:financial institutions], [SUBJECT:Malware Biyomon, RELATION:steals, OBJECT:personal data], [SUBJECT:CyberAttack 2042, RELATION:compromised, OBJECT:database systems],  [SUBJECT:A sample of Biyomon, RELATION:save, OBJECT:a payload]\n",
    "        -The end of the long-term memory area-\n",
    "\n",
    "    -The start of the short-term memory area-  \n",
    "        [SUBJECT:The AgumonHacker Trojan, RELATION:created by, OBJECT:a sophisticated hacker group]\n",
    "    -The end of the short-term memory area-\n",
    "        '''\n",
    "        },\n",
    "    {\"role\": \"assistant\",'content': \n",
    "        '''\n",
    "        Modifications required based on the rules:\n",
    "    Apply Rule 1 and Rule 2: Unify subjects and objects with partially identical terms and remove any prefixes/suffixes/modifiers, particularly for specific names like malware and hacker groups.\n",
    "    Modified short-term memory:\n",
    "\n",
    "    [SUBJECT:The AgumonHacker Trojan, RELATION:created by, OBJECT:a sophisticated hacker group] → [SUBJECT:AgumonHacker, RELATION:created by, OBJECT:sophisticated hacker group]\n",
    "    -The start of new short-term memory area- \n",
    "    [SUBJECT:AgumonHacker, RELATION:created by, OBJECT:sophisticated hacker group], \n",
    "    -The end of new short-term memory area-\n",
    "        '''   },\n",
    "    {\"role\": \"user\",'content': \n",
    "    '''\n",
    "    Good. Now, let's swtich to another article. \n",
    "    -The start of the long-term memory area-\n",
    "    '''+str(longmem)+'''\n",
    "    -The end of the long-term memory area-\n",
    "\n",
    "    -The start of the short-term memory area-\n",
    "    '''+str(shortmem)+'''\n",
    "    -The end of the short-term memory area-\n",
    "    \n",
    "    Now, follow the rules. Write down how you use the rule to modify the one triple in short-term memory. Then, write down new short-term memory which must be started with \\'-The start of new short-term memory area-\\' and ended with \\'-The end of new short-term memory area-\\'\n",
    "    '''\n",
    "    },      \n",
    "    ]\n",
    "    return promptmessage\n",
    "\n",
    "def ask_merge(longmem,shortmem, token, temp, model,streamprint=True):\n",
    "    inpt=generate_prompt(longmem,shortmem,'')\n",
    "    max_retries = 3  # 最大重试次数\n",
    "    retry_count = 0  # 重试计数器\n",
    "    final_shortmem=shortmem\n",
    "    while retry_count < max_retries:\n",
    "        newlongmem=ask(prompt=inpt, token= token, temp=temp, model=model,streamprint=streamprint)\n",
    "        newlongmem=newlongmem.replace('-The start of the new short-term memory area-','-The start of new short-term memory area-')\n",
    "        newlongmem=newlongmem.replace('-The end of the new short-term memory area-','-The end of new short-term memory area-')  \n",
    "        if '-The start of new short-term memory area-' in newlongmem and '-The end of new short-term memory area-' in newlongmem:\n",
    "            newlongmem=newlongmem[newlongmem.rindex('-The start of new short-term memory area-')+len('-The start of new short-term memory area-'):newlongmem.rindex('-The end of new short-term memory area-')]\n",
    "            if not any(keyword in newlongmem for keyword in ['Formbook', 'XLoader', 'savetextfile', 'Leafminer', 'FinSpy', 'Kismet','Agumon','Gabumon','Biyomon','2042']):\n",
    "                final_shortmem=newlongmem\n",
    "                retry_count=9999\n",
    "            else:\n",
    "                retry_count += 1\n",
    "        else:\n",
    "            retry_count += 1\n",
    "    return final_shortmem\n",
    "\n",
    "\n",
    "import re\n",
    "list_triples = []\n",
    "for i in range(1, len(df)):\n",
    "    fulltext = df.iloc[i].single_article\n",
    "    longmemstr = df.iloc[i].longmem\n",
    "    triples = get_triples(longmemstr)\n",
    "    for triple in triples:\n",
    "        sub = triple[0]\n",
    "        rel = triple[1]\n",
    "        obj = triple[2]\n",
    "        subtype = ''\n",
    "        tactic = ''\n",
    "        objtype = ''\n",
    "        simplesen=sub+' '+rel+' '+obj\n",
    "        be=''\n",
    "        #replace 2+ spaces with 1 space\n",
    "        #simplesen=re.sub(' +', ' ', simplesen)\n",
    "        while '  ' in simplesen:\n",
    "            simplesen=simplesen.replace('  ',' ')\n",
    "        list_triples.append([sub, rel, obj, subtype, tactic,be,objtype, simplesen,fulltext,be])\n",
    "        \n",
    "df_triples = pd.DataFrame(list_triples, columns=['sub', 'rel', 'obj', 'subtype', 'tactic','be','objtype','simplesen','fulltext','be'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf_str=''\n",
    "meet_sub_obj=[]\n",
    "for i in range(0, len(df_triples)):\n",
    "    if df_triples.loc[i,'fulltext'] == df_triples.loc[0,'fulltext']:\n",
    "        newdf_str=newdf_str+'['+df_triples.loc[i,'sub']+','+df_triples.loc[i,'rel']+','+df_triples.loc[i,'obj']+']'+'\\n'\n",
    "        meet_sub_obj.append(df_triples.loc[i,'sub'])\n",
    "        meet_sub_obj.append(df_triples.loc[i,'obj'])\n",
    "    else:\n",
    "        sub_name=df_triples.loc[i,'sub']\n",
    "        obj_name=df_triples.loc[i,'obj']\n",
    "        top_5_subs = find_most_similar(sub_name, meet_sub_obj)\n",
    "        top_5_objs = find_most_similar(obj_name, meet_sub_obj)\n",
    "        top_10_subs_objs = list(set(top_5_subs + top_5_objs))\n",
    "        #filter df that sub == top_5_subs or obj == top_5_objs or sub == top_5_objs or obj == top_5_subs\n",
    "        df_similar_overall=[]\n",
    "        df_similar=df_triples[(df_triples['sub'].isin(top_5_subs)) | (df_triples['obj'].isin(top_5_objs)) | (df_triples['sub'].isin(top_5_objs)) | (df_triples['obj'].isin(top_5_subs))]\n",
    "        #remove the duplicates that has same fulltext as current row\n",
    "        df_similar=df_similar[df_similar['fulltext']!=df_triples.loc[i,'fulltext']]\n",
    "        #shuffle the df_similar\n",
    "        df_similar=df_similar.sample(frac=1).reset_index(drop=True)\n",
    "        keep_index=[]\n",
    "        #for df_similar,we only allow 5 rows that share the same sub or obj\n",
    "        for name in top_10_subs_objs:\n",
    "            count=0\n",
    "            for j in range(0,len(df_similar)):\n",
    "                if df_similar.loc[j,'sub']==name or df_similar.loc[j,'obj']==name:\n",
    "                    count+=1\n",
    "                    keep_index.append(j)\n",
    "                if count>=5:\n",
    "                    break\n",
    "        #\n",
    "        df_similar=df_similar.loc[keep_index]\n",
    "        df_similar.reset_index(drop=True, inplace=True)\n",
    "        #build longmemstr by df_similar\n",
    "        longmemstr=''\n",
    "        for j in range(0,len(df_similar)):\n",
    "            longmemstr=longmemstr+'['+df_similar.loc[j,'sub']+','+df_similar.loc[j,'rel']+','+df_similar.loc[j,'obj']+']'+'\\n'\n",
    "        shortmemstr='['+df_triples.loc[i,'sub']+','+df_triples.loc[i,'rel']+','+df_triples.loc[i,'obj']+']'+'\\n'\n",
    "        newline=ask_merge(longmemstr,shortmemstr, 1024, 0.5, \"TheBloke/Yi-34B-Chat-AWQ\",streamprint=False)\n",
    "        cleared=newline.replace('\\n','')\n",
    "        #remove space in the front and end of the string\n",
    "        cleared=cleared.strip()\n",
    "        #use ast to convert string to list\n",
    "        if '[' in cleared and ']' in cleared and cleared.count(',')==2:\n",
    "        #get the first element of the list\n",
    "            newsub=cleared[cleared.index('[')+1:cleared.index(',')].strip()\n",
    "            newrel=cleared[cleared.index(',')+1:cleared.rindex(',')].strip()\n",
    "            newobj=cleared[cleared.rindex(',')+1:cleared.rindex(']')].strip()\n",
    "            #if newsub!=df_triples.loc[i,'sub'] or newobj!=df_triples.loc[i,'obj'] or newrel!=df_triples.loc[i,'rel'], modify the row and print how the row is modified\n",
    "            newsub=newsub.replace('SUBJECT:',\"\")\n",
    "            newobj=newobj.replace('OBJECT:',\"\")\n",
    "            newrel=newrel.replace('RELATION:',\"\")\n",
    "            if newsub!=df_triples.loc[i,'sub'] or newobj!=df_triples.loc[i,'obj'] or newrel!=df_triples.loc[i,'rel']:\n",
    "                print('Before:',df_triples.loc[i,'sub'],df_triples.loc[i,'rel'],df_triples.loc[i,'obj'])\n",
    "                print('After:',newsub,newrel,newobj)\n",
    "                df_triples.loc[i,'sub']=newsub\n",
    "                df_triples.loc[i,'rel']=newrel\n",
    "                df_triples.loc[i,'obj']=newobj\n",
    "                \n",
    "        meet_sub_obj.append(df_triples.loc[i,'sub'])\n",
    "        meet_sub_obj.append(df_triples.loc[i,'obj'])\n",
    "        meet_sub_obj=list(set(meet_sub_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('name_dict.pkl', 'rb') as f:\n",
    "    dict_subobj = pickle.load(f)\n",
    "    \n",
    "# Iterate over each row in df_triples\n",
    "used_subobj=set()\n",
    "for index, row in df_triples.iterrows():\n",
    "    # Check if subtype or objtype is null\n",
    "    \n",
    "    # Check if sub or obj exists in dict_subobj\n",
    "    value=row['sub'].lower()   \n",
    "    if value in dict_subobj.keys():\n",
    "        df_triples.at[index, 'subtype'] = dict_subobj[value]\n",
    "        used_subobj.add(value) \n",
    "        if 'cve' in value and 'c'==value[0]:\n",
    "            df_triples.at[index, 'subtype'] = 'CVE'\n",
    "    #df_triples.at[index, 'subtype'] = dict_subobj[row['sub']]\n",
    "    value=row['obj'].lower()\n",
    "    if value in dict_subobj.keys():\n",
    "        df_triples.at[index, 'objtype'] = dict_subobj[value]\n",
    "        used_subobj.add(value)\n",
    "        if 'cve' in value and 'c'==value[0]:\n",
    "            df_triples.at[index, 'subtype'] = 'CVE'\n",
    "                \n",
    "            #df_triples.at[index, 'objtype'] = dict_subobj[row['obj']]\n",
    "\n",
    "print(len(used_subobj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences=df_triples['simplesen'].tolist()\n",
    "print(len(all_sentences))\n",
    "print(all_sentences[0])\n",
    "#save all_sentences as pkl\n",
    "import pickle\n",
    "with open('all_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(all_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all_sentences.pkl\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs,MultiLabelClassificationModel  # type: ignore\n",
    "import pickle\n",
    "with open('all_sentences.pkl', 'rb') as f:\n",
    "    all_sentences = pickle.load(f)\n",
    "print(len(all_sentences))\n",
    "print(all_sentences[0:3])\n",
    "my_best_model_dir='/content/tactic model'\n",
    "n_model_args = {\n",
    "    \"threshold\": 0.5\n",
    "    }\n",
    "model=MultiLabelClassificationModel(\"roberta\",my_best_model_dir,args=n_model_args)\n",
    "predictions, raw_outputs = model.predict(all_sentences)\n",
    "#read all_sentences.pkl\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs,MultiLabelClassificationModel  # type: ignore\n",
    "import pickle\n",
    "with open('all_sentences.pkl', 'rb') as f:\n",
    "    all_sentences = pickle.load(f)\n",
    "print(len(all_sentences))\n",
    "print(all_sentences[0:3])\n",
    "my_best_model_dir='/content/behavior model'\n",
    "n_model_args = {\n",
    "    \"threshold\": 0.5\n",
    "    }\n",
    "model=ClassificationModel(\"roberta\",my_best_model_dir,args=n_model_args)\n",
    "bepredictions, raw_outputs = model.predict(all_sentences)\n",
    "import pickle\n",
    "with open('all_sentences_tactic.pkl', 'wb') as f:\n",
    "    pickle.dump(predictions, f)\n",
    "import pickle\n",
    "with open('all_sentences_behavior.pkl', 'wb') as f:\n",
    "    pickle.dump(bepredictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('all_sentences_tactic.pkl', 'rb') as f:\n",
    "    all_sentences_tactic = pickle.load(f)\n",
    "df_triples['tactic']=all_sentences_tactic\n",
    "\n",
    "import pickle\n",
    "with open('all_sentences_behavior.pkl', 'rb') as f:\n",
    "    all_sentences_tactic = pickle.load(f)\n",
    "df_triples['be']=all_sentences_tactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=build_graph_alltriples(df_triples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpupower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
