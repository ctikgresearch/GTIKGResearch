{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def compute_full_extracted_triples(intput_sentence):\n",
    "    #v1.7\n",
    "    def generate_prompt(text):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \n",
    "        ''' \n",
    "        As an AI trained in entity extraction and relationship extraction. You're an advanced AI expert, so even if I give you a complex sentence, you'll still be able to perform the relationship extraction task. The output format MUST be a dictionary where key is the source sentence and value is a list consisting of the extracted triple.\n",
    "        A triple is a basic data structure used to represent knowledge graphs, which are structured semantic knowledge bases that describe concepts and their relationships in the physical world. A triple MUST has THREE elements: [Subject, Relation,  Object]. For example, \"[Subject:FinSpy malware, Relation:was the final payload]\"(2 elements) and \"[Subject:FinSpy malware, Relation:was, Object:the final payload, None:that will be used]\"(4 elements) do not contain exactly 3 elements and should be discard.The subject and the object are Noun. The relation is a relation that connects the subject and the object, and expresses how they are related. For example, [Formbook, is, malware] is a triple that describes the relationship between the malware Formbook and the concept of malware. \n",
    "        In entity extraction, you follow those rules:\n",
    "        Rule 1: Only extract triples that are related to cyber attacks. If a sentence does not have any triple about cyber attacks, skip the sentence and do not print it in your output.\\\n",
    "        Rule 2: Make sure your results is a python dictionary format. One example is {source sentence1:[[subject1, relation1, object1],[subject2, relation2, object2]...],source sentence2:[[subject3, relation3, object3],[subject4, relation4, object4]...]} \n",
    "        Rule 3: You must use ellipsis in source sentence to save space. The output format should be  “First word Second word ... penu word last word”, For example, “The malware ... the system”.\n",
    "        '''\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"I got it.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Here is one sentence from example article:\\\"Leafminer attempts to infiltrate target networks through various means of intrusion: watering hole websites, vulnerability scans of network services on the internet, and brute-force/dictionary login attempts.\\\"\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"{Leafminer attempts ... of intrusion: watering hole websites, vulnerability scans of network services on the internet, and brute-force/dictionary login attempts:[[SUBJECT:Leafminer,RELATION:attempts to infiltrate,OBJECT:target networks],[SUBJECT:Leafminer,RELATION:use,OBJECT:watering hole websites],[SUBJECT:Leafminer,RELATION:use,OBJECT:vulnerability scans of network services on the internet],[SUBJECT:Leafminer,RELATION:use,OBJECT:brute-force],[SUBJECT:Leafminer,RELATION:use,OBJECT:dictionary login attempts]]}.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Here is one sentence from example article:\\\"Kismet is also a powerful tool for penetration testers that need to better understand their target and perform wireless LAN discovery.\\\"\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"{Kismet is ... wireless LAN discovery.:[[SUBJECT:Kismet,RELATION:is a powerful tool for, OBJECT:penetration testers],[SUBJECT:testers, RELATION:understand, OBJECT:their target],[SUBJECT:testers,RELATION: perform, OBJECT:wireless LAN discovery]]}.\"\n",
    "        },\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": \n",
    "        \"\"\"\n",
    "        \\Here are my new sentence, extract all possible entity triples from it. Now, I start to give you sentence.\\\"\"\n",
    "        \"\"\"+text+\n",
    "        \"\"\"\\\"Now, my input text are over. You MUST follow the rules I told you before. \n",
    "        \"\"\"\n",
    "        },\n",
    "        ]\n",
    "        return promptmessage\n",
    "\n",
    "    def generate_prompt_basedon3(inSent,inlist):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":'You are responsible for combining the three different entity extraction results from three different assistants extracting from the same sentence into one. The triple is a basic data structure used to represent knowledge graphs, which are structured semantic knowledge bases that describe concepts and their relationships in the world. A triple MUST have THREE elements: [subject, relation, object]. The subject has the prefix \"SUBJECT:\",the relation has prefix \"RELATION:\", the object has prefix \"OBJECT:\", some triples examples are \"[SUBJECT:The user, RELATION:logs in, OBJECT:the system],[SUBJECT:The system, RELATION:stores, personal information],[SUBJECT:The system, RELATION:sends, OBJECT:personal information]\". The final results is a python dictionary format. One example of result is {source sentence1:[[subject1, relation1, object1],[subject2, relation2, object2]...]}. Some assistants use ellipses to simplify words source sentence, for example \"The exploit was delivered through a Microsoft Office document and the final payload was the latest version of FinSpy malware.\" and \"The exploit ... FinSpy malware.\" and \"The exploit was delivered ... latest version of FinSpy malware.\" are the same one sentence. So when you find the different dictionary key that has same beginning and ending words, you should combine them into one dict. I would like you to integrate these three results into one and discard the exact same triples and discard triples that do not contain exactly 3 elements, for example \"[SUBJECT:FinSpy malware, RELATION:was the final payload]\"(2 elements) and \"[SUBJECT:FinSpy malware, RELATION:SUBJECT:was, OBJECT:the final payload, UNKNOWN:that will be used]\"(4 elements) do not contain exactly 3 elements and should be discard. The source sentence is '+str(inSent)+', the extracted triples result are'+str(inlist)+'Just answer me the final python dictionary with triple format without any other words.'\n",
    "        },\n",
    "        ]\n",
    "        return promptmessage\n",
    "    \n",
    "    def generate_prompt_postprocess(text):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \n",
    "        ''' \n",
    "        You play the role of an entity extraction expert and modify/simplify/split the text (extracted multiple triples) in the entity extraction result I gave you (a python dictionary with key as the source sentence with ellipsis and value as the extracted triples) according to the following rules. A triple is a basic data structure used to represent knowledge graphs, which are structured semantic knowledge bases that describe concepts and their relationships in the physical world. A triple consists of three elements: [SUBJECT, RELATION,OBJECT]. The subject and the object are entities, which can be things, people, places, events, or abstract concepts. The relation is a relation that connects the subject and the object, and expresses how they are related. For example, [Formbook, is, malware] is a triple that describes the relationship between the malware Formbook and the concept of malware.\n",
    "        Rule 1: If the subject or object in a triple contains pronouns such as it, they, malware, Trojan, attack, ransomware, or group, replace them with a specific name as much as possible according to the context, such as “CVE-xxx” or “XLoader” will replace \"it\" or \"malware\" if context has this relationship information.\n",
    "        Rule 2: Focus on malware, Trojan horse, CVE, or hacking organization as the subject of the triples, if a subject with \"malware\" or \"Trojan horse\" or \"CVE\" or \"hacking organization\" is found and has additional suffixes, remove the suffixes.\n",
    "        Rule 3: Split a complex triple into multiple simpler forms. For example, [Formbook and XLoader, are,malware] should be split into [Formbook,is,malware] and [XLoader,is,malware].\n",
    "        Rule 4: If the [subject,relation] in a triple can be formed into a new [subject,relation,object] triple because relation itself has a new object in it, create a new triple while keeping the original one. \n",
    "        Rule 5: If the object can be simplified to a more concise, generic expression, create a new triple while keeping the original one. For example, [\"Formbook\", \"save\", \"XLoader in desktop\"] MUST has a new triple [\"Formbook\", \"save\", \"XLoader\"] due to the object \"XLoader in desktop\" can be simplified to \"XLoader\".\n",
    "        Rule 6: Simplify the subject, object, and relation into a more concise, generic expression.\n",
    "        Rule 7:When you encounter a subject or object that contains modifiers and adjectives, remove them. For example, [a notorious Formbook malware] should be simplified to [Formbook].\n",
    "        Rule 8:When you encounter a plural or past tense form, convert it to singular or present tense. For example, [Windows users] should be converted to [Windows user].\n",
    "        Rule 9:When you encounter an MD5, registry, path, or other identifier that contains prefixes, remove them. For example, [md5 xxxxx] should be simplified to [xxxxx].\n",
    "        Rule 10:When you encounter a proper noun that contains a suffix, remove the suffix. For example, [“Specific names of a malware/ransomware/trojan” malware/ransomware/trojan] should be simplified to [“Specific names of a malware/ransomware/trojan”]\n",
    "        Rule 11: Make sure the subject has a prefix \"SUBJECT:\", the relation has prefix \"RELATION:\", the object has prefix \"OBJECT:\", a triple example is \"[SUBJECT:Formbook, RELATION:save, OBJECT:a file]\n",
    "        '''\n",
    "        +\"Here is my entity extraction result:\"+str(text)+\"Now, you apply the rules I told you before. Write down your though, think it step by step. If all triple don't need to be modified based on specific rule, just write down 'no change'.In the end, you MUST tell me the final new entity extraction result. Make sure your results contain a dictionary where key is the original sentence and value is a list consisting of the extracted triple for subsequent information extraction.\"\n",
    "        },\n",
    "        ]\n",
    "        return promptmessage\n",
    "\n",
    "    def get_only_triples(text):\n",
    "        text = text.replace(': [', ':[') \n",
    "        if \"{\" in text and \"}\" in text:\n",
    "            start_index = text.rindex('{')\n",
    "            end_index = text.rindex('}') + 1\n",
    "            triple_only_text = text[start_index:end_index]\n",
    "            if \":[\" in triple_only_text and ']' in triple_only_text:\n",
    "                source_sentence = triple_only_text.split(':')[0] \n",
    "                source_sentence=source_sentence.split('{')[1]\n",
    "                words = source_sentence.split() \n",
    "                if len(words) <= 2:\n",
    "                    abbreviation = source_sentence\n",
    "                else:\n",
    "                    abbreviation = \" \".join(words[:2]) + \" ... \" + \" \".join(words[-2:])\n",
    "                triple_only_text = triple_only_text.replace(source_sentence, abbreviation)     \n",
    "        else:\n",
    "            text = text.replace('\\n', '')\n",
    "            \n",
    "            if \":[\" in text and \"]\" in text:\n",
    "                start_index = text.rindex(':[')\n",
    "                end_index = text.rindex(']') + 1\n",
    "                triple_only_text = text[start_index:end_index]\n",
    "            else:\n",
    "                if \"[[\"in text and \"]]\" in text:\n",
    "                    start_index = text.rindex('[[')\n",
    "                    end_index = text.rindex(']]') + 1\n",
    "                    triple_only_text = text[start_index:end_index]\n",
    "                else:\n",
    "                    triple_only_text = text\n",
    "        return triple_only_text\n",
    "    \n",
    "    def clean_text(text):\n",
    "        import string,re\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        cleaned_text = re.sub(r'[^\\x20-\\x7E]', '', text)\n",
    "        cleaned_text = re.sub(r'[\\s{}]+'.format(re.escape(string.punctuation)), '', cleaned_text)\n",
    "        cleaned_text = re.sub(r'SUBJECT|RELATION|OBJECT', '', cleaned_text)\n",
    "        return cleaned_text if cleaned_text else 'Null'\n",
    "    import os,openai\n",
    "    openai.api_base = \"http://localhost:8080/v1\"\n",
    "    openai.api_key = \"\"\n",
    "    model = \"Empty\"\n",
    "\n",
    "    import concurrent.futures\n",
    "\n",
    "    import re\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "    single_sentence=intput_sentence\n",
    "    import ast\n",
    "    content_first_extraction=''\n",
    "    tried_times=0\n",
    "    redoneflag=True\n",
    "    first_answer_list=[]\n",
    "    temperature_list=[1,0.5,0.2]\n",
    "    \n",
    "    while redoneflag:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=generate_prompt(single_sentence),\n",
    "            max_tokens=8192,\n",
    "            temperature=temperature_list[tried_times],\n",
    "        )\n",
    "        content_first_extraction = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "        cleaned_text = clean_text(str(content_first_extraction))\n",
    "        \n",
    "        if any(keyword in cleaned_text for keyword in ['CVExxx', 'Formbook', 'XLoader', 'Malwaresavetextfile', 'Leafminer', 'FinSpy', 'Kismet', 'Specificnamesofa']):\n",
    "            first_answer_list.append('ERROR')\n",
    "        else:\n",
    "            first_answer_list.append(get_only_triples(content_first_extraction))\n",
    "        tried_times =1+tried_times\n",
    "        if tried_times > 2:\n",
    "            redoneflag = False\n",
    "        else:\n",
    "            redoneflag = True\n",
    "\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=generate_prompt_basedon3(single_sentence,first_answer_list[0:3]),\n",
    "        max_tokens=8192,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    content_first_extraction_merged=completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    content_first_extraction_merged=get_only_triples(content_first_extraction_merged)\n",
    " \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=generate_prompt_postprocess(content_first_extraction_merged),\n",
    "        max_tokens=8192,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    content_simple_version = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "    extracted_text = get_only_triples(content_simple_version)\n",
    "    return extracted_text\n",
    "\n",
    "def clean_full_extracted_triples(text):\n",
    "    #remvoe all \\n\n",
    "    text=text.replace('\\n','')\n",
    "    #remove the space between ‘:\" and \"[\"\n",
    "    import re\n",
    "    text=re.sub(r':\\s+\\[',r'[',text)\n",
    "    #remove the space between ‘[\" and \"[\"\n",
    "    text=re.sub(r'\\s+\\[',r'[',text)\n",
    "    #remove the space between ‘]\" and \"]\"\n",
    "    text=re.sub(r'\\s+\\]',r']',text)\n",
    "    #replace ], ] or ],] or ] ,] with ]]\n",
    "    text = re.sub(r'\\]\\s*,\\s*\\]', ']]', text)\n",
    "\n",
    "    triple_only_text=text\n",
    "    #if [[ and ]] in text, extract the content between them with a [ and ]\n",
    "    if \"[[\" in text and \"]]\" in text:\n",
    "        start_index = text.rindex('[[')+1\n",
    "        end_index = text.rindex(']]') + 1\n",
    "        triple_only_text = text[start_index:end_index]\n",
    "    else:\n",
    "        if \"[[\"in text or \"]]\" in text:\n",
    "            start_index = text.index('[')\n",
    "            end_index = text.rindex(']') + 1\n",
    "            triple_only_text = text[start_index:end_index]\n",
    "    #remove all \" and ' in text\n",
    "    triple_only_text=triple_only_text.replace('\"','')\n",
    "    triple_only_text=triple_only_text.replace(\"'\",'')\n",
    "    #remove all the and The and THE in text\n",
    "    #triple_only_text=triple_only_text.replace('the','')\n",
    "    #triple_only_text=triple_only_text.replace('The','')\n",
    "    #triple_only_text=triple_only_text.replace('THE','')\n",
    "    return triple_only_text\n",
    "\n",
    "def merge_extracted_triples(longmem,shortmem,sentence):\n",
    "    #v1.5\n",
    "    def generate_prompt(longmem,shortmem,sentence):\n",
    "        promptmessage = [\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \n",
    "        '''You are a triples integration assistant. Triple is a basic data structure, which describes concepts and their relationships. A triple in long-term and short-term memory MUST has THREE elements: [Subject, Relation, Object]. You are now reading a whole article and extract all triples from it. But you can only see part of the article at a time. In order to record all the triples from a article, you have the following long-term memory area to record the triples from the entire article. long-term memory stores information on the aricle parts you have already read.\n",
    "        -The start of the long-term memory area-\n",
    "        #Triples will be added here\n",
    "        -The end of the short-term memory area-\n",
    "        Second, you now see a part of this article. Based on this part, you already extract such triples and place them in your short-term memory: \n",
    "        -The start of the short-term memory area-\n",
    "        #Triples will be added here\n",
    "       -The end of the short-term memory area-\n",
    "        Third, now review your long-term memory and short-term memory. Modify the short-term memory into a new short-term memory. You should follow following rules to modify triples in short-term memory to make them consistent with triples in long-term memory. You should write down how you use the rule to modify the triples in short-term memory. In additional, if you find any triples in long-term memory also need to modify based on the rule, you should also write down how you use the rule to modify the triple in long-term memory, and then add new modified triples in short-term memory as a new triple.\n",
    "        \n",
    "        Rule 1. You notice that in these triples, some triples have subjects and objects that contain partially identical terms and refer to the same specific nouns, but these specific nouns have prefixes/suffixes/modifiers that make them not identical. You should delete the prefixes/suffixes/modifiers and unify them into the same specific nouns.\n",
    "        \n",
    "        Before rule: [the Formbook, is designed to run as, a deleter] [Formbook sample, is designed to run as, one-time encryptor]\n",
    "\n",
    "        After rule: [Formbook, is designed to run as, a deleter] [Formbook, is designed to run as, one-time encryptor]\n",
    "\n",
    "        Explanation: The words \"the Formbook\" and \"Formbook sample\" refer to the same entity, so they are unified to use the exact same subject \"Formbook\" for consistency.\n",
    "        \n",
    "        Rule 2. Be especially careful that when you meet specific names of malware,CVE, Trojans, hacker organizations, etc., always use their specific names and remove the prefixes/suffixes/modifiers.\n",
    "        \n",
    "        Before rule: [Malware Formbook, is, malware] \n",
    "        \n",
    "        After rule: [Formbook, is, malware]\n",
    "        \n",
    "        Explanation: The word \"Formbook\" is a specific name of malware, so it should be used as the subject of the triple and the prefix \"Malware\" should be removed.\n",
    "        \n",
    "        Rule 3. Don't add unexisting triples to your new short-term memory. \n",
    "    \n",
    "        Suppose you find in long-term memory: [the malware, download, Leafminer] and in short-term memory: [Formbook, is, malware]. You cannot add a new triple in new short term memory: [Formbook, download, Leafminer]. Because you don't have evidence that \"the malware\" in the long-term memory specifically refers to \"Formbook\".\n",
    "        \n",
    "        Rule 4. Don't add unexisting triples that don't exsit in long-term memory or short-term memory to your new short-term memory. You should add triples from long-term memory or short-term memory to your new short-term memory, not from your imagination and selfcreation\n",
    "        \n",
    "        Rule 5. Don't add any example word like 'Formbook','XLoader','Leafminer', 'FinSpy', 'Kismet' in your new short-term memory area, they are just example words not the real triples in the long term memory area or short term memory area.\n",
    "        \n",
    "        Rule 6. new short-term memory area must be started with \\'-The start of new short-term memory area-\\' and ended with \\'-The end of new short-term memory area-\\'. A triple in new short-term memory MUST has THREE elements: [Subject, Relation, Object]. \n",
    "        '''\n",
    "        },\n",
    "        {\"role\": \"assistant\",'content': 'Yes, I understand and totally follow the rules.'},\n",
    "        {\"role\": \"user\",'content': '''\n",
    "         -The start of the long-term memory area-  \n",
    "         [SUBJECT:AgumonHacker, RELATION:disrupts, OBJECT:network operations], [SUBJECT:HackGroup Gabumon, RELATION:targets, OBJECT:financial institutions], [SUBJECT:Malware Biyomon, RELATION:steals, OBJECT:personal data], [SUBJECT:CyberAttack 2042, RELATION:compromised, OBJECT:database systems],  [SUBJECT:A sample of Biyomon, RELATION:save, OBJECT:a payload]\n",
    "         -The end of the long-term memory area-\n",
    "\n",
    "        -The start of the short-term memory area-  \n",
    "         [SUBJECT:The AgumonHacker Trojan, RELATION:created by, OBJECT:a sophisticated hacker group], [SUBJECT:Gabumon hackers, RELATION:launched, OBJECT:series of phishing campaigns], [SUBJECT:Biyomon malware program, RELATION:encrypts, OBJECT:files for ransom], [SUBJECT:Attack in 2042, RELATION:led to, OBJECT:sensitive leaks],[SUBJECT:Biyomon, RELATION:run, OBJECT:payload]\n",
    "        -The end of the short-term memory area-\n",
    "         '''\n",
    "         },\n",
    "        {\"role\": \"assistant\",'content': \n",
    "         '''\n",
    "         Modifications required based on the rules:\n",
    "        Apply Rule 1 and Rule 2: Unify subjects and objects with partially identical terms and remove any prefixes/suffixes/modifiers, particularly for specific names like malware and hacker groups.\n",
    "        Modified short-term memory:\n",
    "\n",
    "        [SUBJECT:The AgumonHacker Trojan, RELATION:created by, OBJECT:a sophisticated hacker group] → [SUBJECT:AgumonHacker, RELATION:created by, OBJECT:sophisticated hacker group]\n",
    "        [SUBJECT:Gabumon hackers, RELATION:launched, OBJECT:series of phishing campaigns] → [SUBJECT:HackGroup Gabumon, RELATION:launched, OBJECT:phishing campaigns]\n",
    "        [SUBJECT:Biyomon malware program, RELATION:encrypts, OBJECT:files for ransom] → [SUBJECT:Malware Biyomon, RELATION:encrypts, OBJECT:files for ransom]\n",
    "        [SUBJECT:Attack in 2042, RELATION:led to, OBJECT:sensitive leaks] → [SUBJECT:CyberAttack 2042, RELATION:led to, OBJECT:sensitive data leaks]\n",
    "\n",
    "        I also find a triple in long-term memory that needs to be modified based on Rule 1 and Rule 2:\n",
    "\n",
    "        [SUBJECT:A sample of Biyomon, RELATION:save, OBJECT:a payload] → [SUBJECT:Biyomon, RELATION:save, OBJECT:payload]\n",
    "\n",
    "        Apply Rule 3 and 4 and 5: We must ensure we do not create unexisting triples when merging the memories.\n",
    "        After applying the modifications, we now create a new short-term memory area:\n",
    "\n",
    "        -The start of new short-term memory area- \n",
    "        [SUBJECT:AgumonHacker, RELATION:created by, OBJECT:sophisticated hacker group], \n",
    "        [SUBJECT:HackGroup Gabumon, RELATION:launched, OBJECT:phishing campaigns],\n",
    "        [SUBJECT:Biyomon, RELATION:encrypts, OBJECT:files for ransom],\n",
    "        [SUBJECT:CyberAttack 2042, RELATION:led to, OBJECT:sensitive data leaks],\n",
    "        [SUBJECT:Biyomon, RELATION:run, OBJECT:payload],\n",
    "        [SUBJECT:Biyomon, RELATION:save, OBJECT:payload] \n",
    "        -The end of new short-term memory area-\n",
    "         '''   },\n",
    "        {\"role\": \"user\",'content': \n",
    "        '''\n",
    "        Good. Now, let's swtich to another article. \n",
    "        -The start of the long-term memory area-\n",
    "        '''+str(longmem)+'''\n",
    "        -The end of the long-term memory area-\n",
    "    \n",
    "        -The start of the short-term memory area-\n",
    "        '''+str(shortmem)+'''\n",
    "        -The end of the short-term memory area-\n",
    "        \n",
    "        Now, follow the rules. Write down how you use the rule to modify the triples in short-term memory. Then, write down new short-term memory which must be started with \\'-The start of new short-term memory area-\\' and ended with \\'-The end of new short-term memory area-\\'\n",
    "        '''\n",
    "        },      \n",
    "        ]\n",
    "        return promptmessage\n",
    "\n",
    "    import os,openai\n",
    "    openai.api_base = \"http://localhost:8080/v1\"\n",
    "    openai.api_key = \"\"\n",
    "    model = \"Empty\"\n",
    "\n",
    "    import concurrent.futures\n",
    "    def clean_text(text):\n",
    "        import string,re\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        cleaned_text = re.sub(r'[^\\x20-\\x7E]', '', text)\n",
    "        cleaned_text = re.sub(r'[\\s{}]+'.format(re.escape(string.punctuation)), '', cleaned_text)\n",
    "        cleaned_text = re.sub(r'SUBJECT|RELATION|OBJECT', '', cleaned_text)\n",
    "        return cleaned_text if cleaned_text else 'Null'\n",
    "    import re\n",
    "    from tqdm import tqdm\n",
    "    retry_times=0\n",
    "    pass_flag=False\n",
    "   \n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=generate_prompt(longmem,shortmem,sentence),\n",
    "        max_tokens=32768,\n",
    "        temperature=0.7,)\n",
    "    \n",
    "    fullanswer = completion[\"choices\"][0][\"message\"][\"content\"]   \n",
    "         \n",
    "    return fullanswer   \n",
    "\n",
    "def check_brackets(my_string):\n",
    "    if my_string is None or len(my_string) == 0:\n",
    "        return False\n",
    "    my_string = my_string.strip()\n",
    "    first_char_is_bracket = my_string[0] == '['\n",
    "    last_char_is_bracket = my_string[-1] == ']'\n",
    "\n",
    "    if first_char_is_bracket and last_char_is_bracket:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def checker(my_string):\n",
    "    promptmessage = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\":'You are a result checker. You are responsible for checking the result from other AI assistants. The AI assistant may say that \\\" I am sorry, but I am Chat AI model and I am not able to do the task \\\" or \\\" You should do it by yourself\\\" or \\\"I am sorry, but I am not able to do the task\\\". If you found those words or words with simlar meaning, you must reply me \\\"ERROR\\\", other wise, you should reply me \\\"OK\\\". Here is the result from other AI assistant: '+str(my_string)}]\n",
    "    import os\n",
    "    from openai import OpenAI\n",
    "    setmodel='YI'\n",
    "    api_key = \"EMPTY\"\n",
    "    api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=api_base)\n",
    "    stream = client.chat.completions.create(\n",
    "        model=setmodel,\n",
    "        messages=promptmessage,\n",
    "        stream=True,\n",
    "        max_tokens=128,\n",
    "        temperature=1,\n",
    "    )\n",
    "    final_response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            #print(chunk.choices[0].delta.content, end=\"\")\n",
    "            final_response += chunk.choices[0].delta.content  \n",
    "    return final_response \n",
    "\n",
    "def full_text_to_parts(text):\n",
    "    import nltk\n",
    "    # Split paragraphs\n",
    "    paragraphs = text.split('\\n')\n",
    "    \n",
    "    # Initialize result list\n",
    "    processed_paragraphs = []\n",
    "\n",
    "    # Process each paragraph\n",
    "    for paragraph in paragraphs:\n",
    "        if len(paragraph) > 600:\n",
    "            # Use nltk to split sentences\n",
    "            sentences = nltk.sent_tokenize(paragraph)\n",
    "            # Truncate each sentence to 500 characters\n",
    "            sentences = [x[0:500] for x in sentences]\n",
    "            # Initialize new paragraph\n",
    "            new_paragraph = ''\n",
    "            for sentence in sentences:\n",
    "                # Predicted length after merging\n",
    "                temp_length = len(new_paragraph) + len(sentence)\n",
    "                if temp_length < 600:\n",
    "                    # If the length after merging is less than 600, add to the new paragraph\n",
    "                    new_paragraph += (sentence + '\\n')\n",
    "                else:\n",
    "                    # If the new paragraph has at least 20 characters, add it to the result list\n",
    "                    if len(new_paragraph) >= 20:\n",
    "                        processed_paragraphs.append(new_paragraph.strip())\n",
    "                    # Reset new paragraph\n",
    "                    new_paragraph = sentence + '\\n'\n",
    "\n",
    "            # Add the last new paragraph (if there is one and it has at least 20 characters)\n",
    "            if len(new_paragraph) >= 20:\n",
    "                processed_paragraphs.append(new_paragraph.strip())\n",
    "        else:\n",
    "            if len(paragraph) >= 20:\n",
    "                processed_paragraphs.append(paragraph.strip())\n",
    "\n",
    "    # Merge shorter paragraphs\n",
    "    combined_paragraphs = []\n",
    "    current_combined = ''\n",
    "\n",
    "    for paragraph in processed_paragraphs:\n",
    "        # Calculate the potential length of the current merged paragraph\n",
    "        temp_length = len(current_combined) + len(paragraph) + 1  # Add 1 because there is a space between paragraphs\n",
    "        if temp_length < 600:\n",
    "            # If the length after merging is less than 600, continue to merge\n",
    "            current_combined += (' ' if current_combined else '') + paragraph\n",
    "        else:\n",
    "            # Otherwise, store the current merged paragraph and reset\n",
    "            combined_paragraphs.append(current_combined)\n",
    "            current_combined = paragraph\n",
    "            \n",
    "    # Add the last merged paragraph (if there is one)\n",
    "    if current_combined:\n",
    "        combined_paragraphs.append(current_combined)\n",
    "\n",
    "    return combined_paragraphs\n",
    "\n",
    "def full_article_to_longmem(single_article):\n",
    "    grouped_texts_strings = full_text_to_parts(single_article)\n",
    "    triple_cache = []\n",
    "    text_cache = []\n",
    "    for i in range(len(grouped_texts_strings)):\n",
    "        this_time_test=grouped_texts_strings[i]\n",
    "        if len(this_time_test) > 1500:\n",
    "            this_time_test = this_time_test[0:1500]\n",
    "        print('Thinking about paragraph '+str(i))\n",
    "        print('Seeing text：',this_time_test)\n",
    "        triple = compute_full_extracted_triples(this_time_test)\n",
    "        clean_triple_forMEM = clean_full_extracted_triples(triple)\n",
    "\n",
    "        if  'Formbook' in clean_triple_forMEM or 'XLoader' in clean_triple_forMEM or 'savetextfile' in clean_triple_forMEM or 'Leafminer' in clean_triple_forMEM or 'FinSpy' in clean_triple_forMEM or 'Kismet' in clean_triple_forMEM or 'Agumon' in clean_triple_forMEM or 'Gabumon' in clean_triple_forMEM or 'Biyomon' in clean_triple_forMEM or '2042' in clean_triple_forMEM or check_brackets(clean_triple_forMEM)==False or checker(triple)=='ERROR':\n",
    "            print('Current short-term memory does not meet requirements',triple)\n",
    "            print('Retry extracting text 1')\n",
    "            triple = compute_full_extracted_triples(this_time_test)\n",
    "            clean_triple_forMEM = clean_full_extracted_triples(triple)\n",
    "\n",
    "        if  'Formbook' in clean_triple_forMEM or 'XLoader' in clean_triple_forMEM or 'savetextfile' in clean_triple_forMEM or 'Leafminer' in clean_triple_forMEM or 'FinSpy' in clean_triple_forMEM or 'Kismet' in clean_triple_forMEM or 'Agumon' in clean_triple_forMEM or 'Gabumon' in clean_triple_forMEM or 'Biyomon' in clean_triple_forMEM or '2042' in clean_triple_forMEM or check_brackets(clean_triple_forMEM)==False or checker(triple)=='ERROR':\n",
    "            print('Current short-term memory does not meet requirements',triple)\n",
    "            print('Retry extracting text 2')\n",
    "            triple = compute_full_extracted_triples(this_time_test)\n",
    "            clean_triple_forMEM = clean_full_extracted_triples(triple)\n",
    "        \n",
    "        if  'Formbook' in clean_triple_forMEM or 'XLoader' in clean_triple_forMEM or 'savetextfile' in clean_triple_forMEM or 'Leafminer' in clean_triple_forMEM or 'FinSpy' in clean_triple_forMEM or 'Kismet' in clean_triple_forMEM or 'Agumon' in clean_triple_forMEM or 'Gabumon' in clean_triple_forMEM or 'Biyomon' in clean_triple_forMEM or '2042' in clean_triple_forMEM or check_brackets(clean_triple_forMEM)==False or checker(triple)=='ERROR':\n",
    "            print('Current short-term memory does not meet requirements',triple)\n",
    "            print('Retry extracting text 3')\n",
    "            triple = compute_full_extracted_triples(this_time_test)\n",
    "            clean_triple_forMEM = triple\n",
    "            \n",
    "        print('This time short-term memory is:')\n",
    "        print(clean_triple_forMEM)\n",
    "        \n",
    "        if i == 0:\n",
    "            if check_brackets(clean_triple_forMEM):\n",
    "                longmem = clean_triple_forMEM\n",
    "            else:\n",
    "                longmem = 'No longterm memory'\n",
    "            triple_cache.append(clean_triple_forMEM)\n",
    "            text_cache.append(this_time_test)\n",
    "            print('First thinking completed')\n",
    "        if i >= 1:\n",
    "            print('Past long-term memory is:')\n",
    "            print(longmem)\n",
    "            original_longmem=longmem\n",
    "            if len(longmem)>=1500:\n",
    "                    longmem=longmem[-1000:]\n",
    "                    if '[' in longmem:\n",
    "                        longmem=longmem[longmem.index('['):]\n",
    "            if check_brackets(clean_triple_forMEM):\n",
    "                max_retries = 3  # Maximum retry times\n",
    "                retry_count = 0  # Retry counter\n",
    "                while retry_count < max_retries:\n",
    "                    print('Retry '+str(retry_count)+' times')\n",
    "                    newlongmem = merge_extracted_triples(longmem, clean_triple_forMEM, this_time_test)\n",
    "                    print('Thinking process：')\n",
    "                    print(newlongmem)\n",
    "                    newlongmem=newlongmem.replace('-The start of the new short-term memory area-','-The start of new short-term memory area-')\n",
    "                    newlongmem=newlongmem.replace('-The end of the new short-term memory area-','-The end of new short-term memory area-') \n",
    "                    if '-The start of new short-term memory area-' in newlongmem and '-The end of new short-term memory area-' in newlongmem and checker(newlongmem)!='ERROR':\n",
    "                        newlongmem=newlongmem[newlongmem.rindex('-The start of new short-term memory area-')+len('-The start of new short-term memory area-'):newlongmem.rindex('-The end of new short-term memory area-')]\n",
    "                        if not any(keyword in newlongmem for keyword in ['Formbook', 'XLoader', 'savetextfile', 'Leafminer', 'FinSpy', 'Kismet','Agumon','Gabumon','Biyomon','2042']):\n",
    "                            longmem = str(original_longmem)+', '+str(newlongmem)\n",
    "                            retry_count=9999\n",
    "                        else:\n",
    "                            retry_count += 1\n",
    "                    else:\n",
    "                        retry_count += 1\n",
    "            else:\n",
    "                longmem=original_longmem\n",
    "                print('Short-term memory is not a triple')\n",
    "            print('After merging: The new long-term memory is:')\n",
    "            print(longmem)      \n",
    "            import pandas as pd\n",
    "\n",
    "            # Create a new DataFrame\n",
    "            new_data = pd.DataFrame({'single_article': [str(single_article)], 'longmem': [str(longmem),]})\n",
    "\n",
    "            try:\n",
    "                # Read the existing Excel file\n",
    "                longmem_cache = pd.read_excel('RQ2 result cache backup.xlsx')\n",
    "                # Add new data to the end of existing data\n",
    "                longmem_cache = pd.concat([longmem_cache, new_data], ignore_index=True)\n",
    "            except FileNotFoundError:\n",
    "                # If the file does not exist, use the new data directly\n",
    "                longmem_cache = new_data\n",
    "\n",
    "            # Save the updated data to the Excel file\n",
    "            longmem_cache.to_excel('RQ2 result cache backup.xlsx', index=False)\n",
    "            # Add the result to the cache\n",
    "            \n",
    "    return longmem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "target= pd.read_csv('target.csv')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_sentence(sentence, index):\n",
    "    try:\n",
    "        full_article_to_longmem(sentence)\n",
    "        with open(\"done.txt\", \"a\") as file:\n",
    "            file.write(sentence + \"\\n\")\n",
    "        with open(\"done_index.txt\", \"w\") as file:\n",
    "            file.write(str(index))\n",
    "    except Exception as e:\n",
    "        with open(\"error.txt\", \"a\") as file:\n",
    "            file.write(sentence + \"\\n\")\n",
    "        print(f\"Error：{e}\")\n",
    "\n",
    "emotet_sentences = target['string'].tolist()\n",
    "\n",
    "pool = ThreadPoolExecutor(max_workers=16)\n",
    "\n",
    "for i, sentence in enumerate(emotet_sentences):\n",
    "    try:\n",
    "        pool.submit(process_sentence, sentence, i)\n",
    "    except Exception as e:\n",
    "        print(f\"Error：{e}\")\n",
    "        continue\n",
    "\n",
    "pool.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpupower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
